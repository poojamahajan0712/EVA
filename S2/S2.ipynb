{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EVA4 - Session 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch                   ## importing pytorch   \n",
        "import torch.nn as nn  ##PyTorch provides the torch.nn module to help us in creating and  training of the neural network \n",
        "\n",
        "import torch.nn.functional as F ## The torch.nn.functional area specifically gives us access to some handy functions that we might not want to write ourselves. We will be using the relu or \"rectified linear\" activation function for our neurons. Instead of writing all of the code for these things, we can just import them, since these are things everyone will be needing in their deep learning code. \n",
        "import torch.optim as optim  ###torch.optim is a package implementing various optimization algorithms   \n",
        "from torchvision import datasets, transforms  ###The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()       ##super constructor – call constructor of nn.Module \n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  #input –1  OUtput – 32  RF-3 \n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #input –32  OUtput – 64  RF-5 \n",
        "        self.pool1 = nn.MaxPool2d(2, 2)     #        RF10        \n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1) #input –64  OUtput – 128  RF-12 \n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)#input –128  OUtput – 256  RF-14 \n",
        "        self.pool2 = nn.MaxPool2d(2, 2)   # RF 28 \n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)#input –256  OUtput – 512  RF-30 \n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3) #input –512  OUtput – 1024  RF-32 \n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3) #input –1024  OUtput – 10  RF-34 \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))                #The forward function computes output Tensors from input Tensors.\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x = self.conv7(self.conv6(F.relu(self.conv5(x))))\n",
        "        #x = F.relu(self.conv7(x))\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xdydjYTZFyi3",
        "outputId": "6569205a-a0cd-48e5-a548-29a6d3b26347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))\n",
        "\n",
        "#### basically a summary of the above function defined 32 ,64,64,128 … respectively is no,. Of channels in output layer . \n",
        "\n",
        "##While 28*28 , 14*14  is the padded value of image size in the layers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab_type": "code",
        "outputId": "8f2b39a2-40a9-4e9b-c582-ed62457e7d91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "'''loading data – train ,test . Doing transformation by converting to tensor and normalisation. \n",
        "\n",
        "num_workers (int, optional): how many subprocesses to use for data loading. \n",
        "\n",
        "pin_memory (bool, optional): If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them.  \n",
        "\n",
        "Transforms.Compose :- Composes several transforms together. \n",
        "\n",
        " \n",
        "\n",
        "torchvision.transforms.Normalize(mean, std, inplace=False)[SOURCE] \n",
        "\n",
        "Normalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input  \n",
        "\n",
        " \n",
        "\n",
        "torchvision.transforms.ToTensor[SOURCE] \n",
        "\n",
        "Convert a PIL Image or numpy.ndarray to tensor. \n",
        "\n",
        "Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \n",
        "'''\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'loading data – train ,test . Doing transformation by converting to tensor and normalisation. \\n\\nnum_workers (int, optional): how many subprocesses to use for data loading. \\n\\npin_memory (bool, optional): If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them.  \\n\\nTransforms.Compose :- Composes several transforms together. \\n\\n \\n\\ntorchvision.transforms.Normalize(mean, std, inplace=False)[SOURCE] \\n\\nNormalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input  \\n\\n \\n\\ntorchvision.transforms.ToTensor[SOURCE] \\n\\nConvert a PIL Image or numpy.ndarray to tensor. \\n\\nConverts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH",
        "colab_type": "code",
        "outputId": "81003150-4a71-4e21-e4d2-546f52c029a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from tqdm import tqdm                  \n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "\n",
        "''' \n",
        "\n",
        "model.train() tells your model that you are training the model. So effectively layers like dropout, batchnorm etc. which behave different on the train and test procedures know what is going on and hence can behave accordingly. \n",
        "\n",
        "More details: It sets the mode to train (see source code). You can call either model.eval() or model.train(mode=False) to tell that you are testing. It is somewhat intuitive to expect train function to train model but it does not do that. It just sets the mode. \n",
        "\n",
        "      optimizer.zero_grad() \n",
        "\n",
        "Since the backward() function accumulates gradients, and you don’t want to mix up gradients between minibatches, you have to zero them out at the start of a new minibatch. This is exactly like how a general (additive) accumulator variable is initialized to 0 in code. \n",
        "\n",
        "By the way, the best practice is to use the zero_grad() 2.8k function on the optimizer. \n",
        "\n",
        " \n",
        "\n",
        "optimizer.step is performs a parameter update based on the current gradient (stored in .grad attribute of a parameter) and the update rule. As an example, the update rule for SGD is defined here: \n",
        "https://github.com/pytorch/pytorch/blob/cd9b27231b51633e76e28b6a34002ab83b0660fc/torch/optim/sgd.py#L63 2.3k. \n",
        "\n",
        "Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter. This is why you should call optimizer.zero_grad() after each .step() call. Note that following the first .backward call, a second call is only possible after you have performed another forward pass. \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\n\\nmodel.train() tells your model that you are training the model. So effectively layers like dropout, batchnorm etc. which behave different on the train and test procedures know what is going on and hence can behave accordingly. \\n\\nMore details: It sets the mode to train (see source code). You can call either model.eval() or model.train(mode=False) to tell that you are testing. It is somewhat intuitive to expect train function to train model but it does not do that. It just sets the mode. \\n\\n      optimizer.zero_grad() \\n\\nSince the backward() function accumulates gradients, and you don’t want to mix up gradients between minibatches, you have to zero them out at the start of a new minibatch. This is exactly like how a general (additive) accumulator variable is initialized to 0 in code. \\n\\nBy the way, the best practice is to use the zero_grad() 2.8k function on the optimizer. \\n\\n \\n\\noptimizer.step is performs a parameter update based on the current gradient (stored in .grad attribute of a parameter) and the update rule. As an example, the update rule for SGD is defined here: \\nhttps://github.com/pytorch/pytorch/blob/cd9b27231b51633e76e28b6a34002ab83b0660fc/torch/optim/sgd.py#L63 2.3k. \\n\\nCalling .backward() mutiple times accumulates the gradient (by addition) for each parameter. This is why you should call optimizer.zero_grad() after each .step() call. Note that following the first .backward call, a second call is only possible after you have performed another forward pass. \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab_type": "code",
        "outputId": "fe2486fa-6d6f-45d8-ec79-918421e1fdeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "model= Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "    #Lr is learning rate  \n",
        "\n",
        "#Momentum – momentum factor "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\n",
            "loss=2.3033065795898438 batch_id=0:   0%|          | 0/469 [00:00<?, ?it/s]\u001b[A\n",
            "loss=2.3033065795898438 batch_id=0:   0%|          | 1/469 [00:00<00:49,  9.50it/s]\u001b[A\n",
            "loss=2.300729274749756 batch_id=1:   0%|          | 1/469 [00:00<00:49,  9.50it/s] \u001b[A\n",
            "loss=2.3013687133789062 batch_id=2:   0%|          | 1/469 [00:00<00:49,  9.50it/s]\u001b[A\n",
            "loss=2.299576759338379 batch_id=3:   0%|          | 1/469 [00:00<00:49,  9.50it/s] \u001b[A\n",
            "loss=2.299576759338379 batch_id=3:   1%|          | 4/469 [00:00<00:40, 11.50it/s]\u001b[A\n",
            "loss=2.302168369293213 batch_id=4:   1%|          | 4/469 [00:00<00:40, 11.50it/s]\u001b[A\n",
            "loss=2.2996318340301514 batch_id=5:   1%|          | 4/469 [00:00<00:40, 11.50it/s]\u001b[A\n",
            "loss=2.2996318340301514 batch_id=5:   1%|▏         | 6/469 [00:00<00:35, 13.14it/s]\u001b[A\n",
            "loss=2.298245429992676 batch_id=6:   1%|▏         | 6/469 [00:00<00:35, 13.14it/s] \u001b[A\n",
            "loss=2.2952821254730225 batch_id=7:   1%|▏         | 6/469 [00:00<00:35, 13.14it/s]\u001b[A\n",
            "loss=2.2946512699127197 batch_id=8:   1%|▏         | 6/469 [00:00<00:35, 13.14it/s]\u001b[A\n",
            "loss=2.2946512699127197 batch_id=8:   2%|▏         | 9/469 [00:00<00:30, 15.19it/s]\u001b[A\n",
            "loss=2.298283576965332 batch_id=9:   2%|▏         | 9/469 [00:00<00:30, 15.19it/s] \u001b[A\n",
            "loss=2.2950284481048584 batch_id=10:   2%|▏         | 9/469 [00:00<00:30, 15.19it/s]\u001b[A\n",
            "loss=2.2950284481048584 batch_id=10:   2%|▏         | 11/469 [00:00<00:28, 16.18it/s]\u001b[A\n",
            "loss=2.2918787002563477 batch_id=11:   2%|▏         | 11/469 [00:00<00:28, 16.18it/s]\u001b[A\n",
            "loss=2.29345440864563 batch_id=12:   2%|▏         | 11/469 [00:00<00:28, 16.18it/s]  \u001b[A\n",
            "loss=2.29345440864563 batch_id=12:   3%|▎         | 13/469 [00:00<00:26, 17.07it/s]\u001b[A\n",
            "loss=2.2864456176757812 batch_id=13:   3%|▎         | 13/469 [00:00<00:26, 17.07it/s]\u001b[A\n",
            "loss=2.2804079055786133 batch_id=14:   3%|▎         | 13/469 [00:00<00:26, 17.07it/s]\u001b[A\n",
            "loss=2.2822561264038086 batch_id=15:   3%|▎         | 13/469 [00:00<00:26, 17.07it/s]\u001b[A\n",
            "loss=2.2822561264038086 batch_id=15:   3%|▎         | 16/469 [00:00<00:24, 18.34it/s]\u001b[A\n",
            "loss=2.292753219604492 batch_id=16:   3%|▎         | 16/469 [00:00<00:24, 18.34it/s] \u001b[A\n",
            "loss=2.2773361206054688 batch_id=17:   3%|▎         | 16/469 [00:00<00:24, 18.34it/s]\u001b[A\n",
            "loss=2.2799792289733887 batch_id=18:   3%|▎         | 16/469 [00:00<00:24, 18.34it/s]\u001b[A\n",
            "loss=2.2799792289733887 batch_id=18:   4%|▍         | 19/469 [00:00<00:23, 19.08it/s]\u001b[A\n",
            "loss=2.283755302429199 batch_id=19:   4%|▍         | 19/469 [00:00<00:23, 19.08it/s] \u001b[A\n",
            "loss=2.286905527114868 batch_id=20:   4%|▍         | 19/469 [00:01<00:23, 19.08it/s]\u001b[A\n",
            "loss=2.286905527114868 batch_id=20:   4%|▍         | 21/469 [00:01<00:23, 19.31it/s]\u001b[A\n",
            "loss=2.268876791000366 batch_id=21:   4%|▍         | 21/469 [00:01<00:23, 19.31it/s]\u001b[A\n",
            "loss=2.26027774810791 batch_id=22:   4%|▍         | 21/469 [00:01<00:23, 19.31it/s] \u001b[A\n",
            "loss=2.26027774810791 batch_id=22:   5%|▍         | 23/469 [00:01<00:22, 19.50it/s]\u001b[A\n",
            "loss=2.258744239807129 batch_id=23:   5%|▍         | 23/469 [00:01<00:22, 19.50it/s]\u001b[A\n",
            "loss=2.253021001815796 batch_id=24:   5%|▍         | 23/469 [00:01<00:22, 19.50it/s]\u001b[A\n",
            "loss=2.253021001815796 batch_id=24:   5%|▌         | 25/469 [00:01<00:22, 19.38it/s]\u001b[A\n",
            "loss=2.256788730621338 batch_id=25:   5%|▌         | 25/469 [00:01<00:22, 19.38it/s]\u001b[A\n",
            "loss=2.251842498779297 batch_id=26:   5%|▌         | 25/469 [00:01<00:22, 19.38it/s]\u001b[A\n",
            "loss=2.232293128967285 batch_id=27:   5%|▌         | 25/469 [00:01<00:22, 19.38it/s]\u001b[A\n",
            "loss=2.232293128967285 batch_id=27:   6%|▌         | 28/469 [00:01<00:22, 19.69it/s]\u001b[A\n",
            "loss=2.2181992530822754 batch_id=28:   6%|▌         | 28/469 [00:01<00:22, 19.69it/s]\u001b[A\n",
            "loss=2.207184076309204 batch_id=29:   6%|▌         | 28/469 [00:01<00:22, 19.69it/s] \u001b[A\n",
            "loss=2.2141482830047607 batch_id=30:   6%|▌         | 28/469 [00:01<00:22, 19.69it/s]\u001b[A\n",
            "loss=2.2141482830047607 batch_id=30:   7%|▋         | 31/469 [00:01<00:21, 20.34it/s]\u001b[A\n",
            "loss=2.219743251800537 batch_id=31:   7%|▋         | 31/469 [00:01<00:21, 20.34it/s] \u001b[A\n",
            "loss=2.1714162826538086 batch_id=32:   7%|▋         | 31/469 [00:01<00:21, 20.34it/s]\u001b[A\n",
            "loss=2.122267007827759 batch_id=33:   7%|▋         | 31/469 [00:01<00:21, 20.34it/s] \u001b[A\n",
            "loss=2.122267007827759 batch_id=33:   7%|▋         | 34/469 [00:01<00:21, 20.60it/s]\u001b[A\n",
            "loss=2.139272451400757 batch_id=34:   7%|▋         | 34/469 [00:01<00:21, 20.60it/s]\u001b[A\n",
            "loss=2.0634336471557617 batch_id=35:   7%|▋         | 34/469 [00:01<00:21, 20.60it/s]\u001b[A\n",
            "loss=2.040449619293213 batch_id=36:   7%|▋         | 34/469 [00:01<00:21, 20.60it/s] \u001b[A\n",
            "loss=2.040449619293213 batch_id=36:   8%|▊         | 37/469 [00:01<00:20, 20.68it/s]\u001b[A\n",
            "loss=1.9751137495040894 batch_id=37:   8%|▊         | 37/469 [00:01<00:20, 20.68it/s]\u001b[A\n",
            "loss=1.8361966609954834 batch_id=38:   8%|▊         | 37/469 [00:01<00:20, 20.68it/s]\u001b[A\n",
            "loss=1.7199785709381104 batch_id=39:   8%|▊         | 37/469 [00:01<00:20, 20.68it/s]\u001b[A\n",
            "loss=1.7199785709381104 batch_id=39:   9%|▊         | 40/469 [00:01<00:20, 21.36it/s]\u001b[A\n",
            "loss=1.6433625221252441 batch_id=40:   9%|▊         | 40/469 [00:02<00:20, 21.36it/s]\u001b[A\n",
            "loss=1.3460971117019653 batch_id=41:   9%|▊         | 40/469 [00:02<00:20, 21.36it/s]\u001b[A\n",
            "loss=1.1843761205673218 batch_id=42:   9%|▊         | 40/469 [00:02<00:20, 21.36it/s]\u001b[A\n",
            "loss=1.1843761205673218 batch_id=42:   9%|▉         | 43/469 [00:02<00:19, 21.68it/s]\u001b[A\n",
            "loss=1.1098705530166626 batch_id=43:   9%|▉         | 43/469 [00:02<00:19, 21.68it/s]\u001b[A\n",
            "loss=0.8750839829444885 batch_id=44:   9%|▉         | 43/469 [00:02<00:19, 21.68it/s]\u001b[A\n",
            "loss=0.8985447883605957 batch_id=45:   9%|▉         | 43/469 [00:02<00:19, 21.68it/s]\u001b[A\n",
            "loss=0.8985447883605957 batch_id=45:  10%|▉         | 46/469 [00:02<00:19, 22.12it/s]\u001b[A\n",
            "loss=0.9896376729011536 batch_id=46:  10%|▉         | 46/469 [00:02<00:19, 22.12it/s]\u001b[A\n",
            "loss=1.0523719787597656 batch_id=47:  10%|▉         | 46/469 [00:02<00:19, 22.12it/s]\u001b[A\n",
            "loss=0.9694421887397766 batch_id=48:  10%|▉         | 46/469 [00:02<00:19, 22.12it/s]\u001b[A\n",
            "loss=0.9694421887397766 batch_id=48:  10%|█         | 49/469 [00:02<00:18, 22.43it/s]\u001b[A\n",
            "loss=0.8720107078552246 batch_id=49:  10%|█         | 49/469 [00:02<00:18, 22.43it/s]\u001b[A\n",
            "loss=0.7814128398895264 batch_id=50:  10%|█         | 49/469 [00:02<00:18, 22.43it/s]\u001b[A\n",
            "loss=0.5333850979804993 batch_id=51:  10%|█         | 49/469 [00:02<00:18, 22.43it/s]\u001b[A\n",
            "loss=0.5333850979804993 batch_id=51:  11%|█         | 52/469 [00:02<00:18, 22.19it/s]\u001b[A\n",
            "loss=0.6072991490364075 batch_id=52:  11%|█         | 52/469 [00:02<00:18, 22.19it/s]\u001b[A\n",
            "loss=0.5826129913330078 batch_id=53:  11%|█         | 52/469 [00:02<00:18, 22.19it/s]\u001b[A\n",
            "loss=0.7333877086639404 batch_id=54:  11%|█         | 52/469 [00:02<00:18, 22.19it/s]\u001b[A\n",
            "loss=0.7333877086639404 batch_id=54:  12%|█▏        | 55/469 [00:02<00:19, 20.76it/s]\u001b[A\n",
            "loss=0.5115076899528503 batch_id=55:  12%|█▏        | 55/469 [00:02<00:19, 20.76it/s]\u001b[A\n",
            "loss=0.6336383819580078 batch_id=56:  12%|█▏        | 55/469 [00:02<00:19, 20.76it/s]\u001b[A\n",
            "loss=0.5155634880065918 batch_id=57:  12%|█▏        | 55/469 [00:02<00:19, 20.76it/s]\u001b[A\n",
            "loss=0.5155634880065918 batch_id=57:  12%|█▏        | 58/469 [00:02<00:19, 21.22it/s]\u001b[A\n",
            "loss=0.5102443695068359 batch_id=58:  12%|█▏        | 58/469 [00:02<00:19, 21.22it/s]\u001b[A\n",
            "loss=0.6498458385467529 batch_id=59:  12%|█▏        | 58/469 [00:02<00:19, 21.22it/s]\u001b[A\n",
            "loss=0.5733703970909119 batch_id=60:  12%|█▏        | 58/469 [00:02<00:19, 21.22it/s]\u001b[A\n",
            "loss=0.5733703970909119 batch_id=60:  13%|█▎        | 61/469 [00:02<00:19, 21.00it/s]\u001b[A\n",
            "loss=0.481935054063797 batch_id=61:  13%|█▎        | 61/469 [00:02<00:19, 21.00it/s] \u001b[A\n",
            "loss=0.5053809881210327 batch_id=62:  13%|█▎        | 61/469 [00:03<00:19, 21.00it/s]\u001b[A\n",
            "loss=0.4582117795944214 batch_id=63:  13%|█▎        | 61/469 [00:03<00:19, 21.00it/s]\u001b[A\n",
            "loss=0.4582117795944214 batch_id=63:  14%|█▎        | 64/469 [00:03<00:18, 21.34it/s]\u001b[A\n",
            "loss=0.3232002854347229 batch_id=64:  14%|█▎        | 64/469 [00:03<00:18, 21.34it/s]\u001b[A\n",
            "loss=0.3937501311302185 batch_id=65:  14%|█▎        | 64/469 [00:03<00:18, 21.34it/s]\u001b[A\n",
            "loss=0.3858768045902252 batch_id=66:  14%|█▎        | 64/469 [00:03<00:18, 21.34it/s]\u001b[A\n",
            "loss=0.3858768045902252 batch_id=66:  14%|█▍        | 67/469 [00:03<00:18, 21.65it/s]\u001b[A\n",
            "loss=0.5729386806488037 batch_id=67:  14%|█▍        | 67/469 [00:03<00:18, 21.65it/s]\u001b[A\n",
            "loss=0.5474008917808533 batch_id=68:  14%|█▍        | 67/469 [00:03<00:18, 21.65it/s]\u001b[A\n",
            "loss=0.3576756715774536 batch_id=69:  14%|█▍        | 67/469 [00:03<00:18, 21.65it/s]\u001b[A\n",
            "loss=0.3576756715774536 batch_id=69:  15%|█▍        | 70/469 [00:03<00:18, 21.92it/s]\u001b[A\n",
            "loss=0.5824432373046875 batch_id=70:  15%|█▍        | 70/469 [00:03<00:18, 21.92it/s]\u001b[A\n",
            "loss=0.23687276244163513 batch_id=71:  15%|█▍        | 70/469 [00:03<00:18, 21.92it/s]\u001b[A\n",
            "loss=0.3199771046638489 batch_id=72:  15%|█▍        | 70/469 [00:03<00:18, 21.92it/s] \u001b[A\n",
            "loss=0.3199771046638489 batch_id=72:  16%|█▌        | 73/469 [00:03<00:18, 21.87it/s]\u001b[A\n",
            "loss=0.35827401280403137 batch_id=73:  16%|█▌        | 73/469 [00:03<00:18, 21.87it/s]\u001b[A\n",
            "loss=0.4135949909687042 batch_id=74:  16%|█▌        | 73/469 [00:03<00:18, 21.87it/s] \u001b[A\n",
            "loss=0.3606025278568268 batch_id=75:  16%|█▌        | 73/469 [00:03<00:18, 21.87it/s]\u001b[A\n",
            "loss=0.3606025278568268 batch_id=75:  16%|█▌        | 76/469 [00:03<00:18, 21.83it/s]\u001b[A\n",
            "loss=0.46402621269226074 batch_id=76:  16%|█▌        | 76/469 [00:03<00:18, 21.83it/s]\u001b[A\n",
            "loss=0.4969910681247711 batch_id=77:  16%|█▌        | 76/469 [00:03<00:18, 21.83it/s] \u001b[A\n",
            "loss=0.4401017725467682 batch_id=78:  16%|█▌        | 76/469 [00:03<00:18, 21.83it/s]\u001b[A\n",
            "loss=0.4401017725467682 batch_id=78:  17%|█▋        | 79/469 [00:03<00:17, 21.94it/s]\u001b[A\n",
            "loss=0.36166122555732727 batch_id=79:  17%|█▋        | 79/469 [00:03<00:17, 21.94it/s]\u001b[A\n",
            "loss=0.3521365821361542 batch_id=80:  17%|█▋        | 79/469 [00:03<00:17, 21.94it/s] \u001b[A\n",
            "loss=0.4016338288784027 batch_id=81:  17%|█▋        | 79/469 [00:03<00:17, 21.94it/s]\u001b[A\n",
            "loss=0.4016338288784027 batch_id=81:  17%|█▋        | 82/469 [00:03<00:17, 21.98it/s]\u001b[A\n",
            "loss=0.3514101505279541 batch_id=82:  17%|█▋        | 82/469 [00:03<00:17, 21.98it/s]\u001b[A\n",
            "loss=0.33605581521987915 batch_id=83:  17%|█▋        | 82/469 [00:03<00:17, 21.98it/s]\u001b[A\n",
            "loss=0.43211933970451355 batch_id=84:  17%|█▋        | 82/469 [00:04<00:17, 21.98it/s]\u001b[A\n",
            "loss=0.43211933970451355 batch_id=84:  18%|█▊        | 85/469 [00:04<00:17, 22.16it/s]\u001b[A\n",
            "loss=0.2725566625595093 batch_id=85:  18%|█▊        | 85/469 [00:04<00:17, 22.16it/s] \u001b[A\n",
            "loss=0.3141242265701294 batch_id=86:  18%|█▊        | 85/469 [00:04<00:17, 22.16it/s]\u001b[A\n",
            "loss=0.23102222383022308 batch_id=87:  18%|█▊        | 85/469 [00:04<00:17, 22.16it/s]\u001b[A\n",
            "loss=0.23102222383022308 batch_id=87:  19%|█▉        | 88/469 [00:04<00:16, 22.86it/s]\u001b[A\n",
            "loss=0.4751470685005188 batch_id=88:  19%|█▉        | 88/469 [00:04<00:16, 22.86it/s] \u001b[A\n",
            "loss=0.3066018521785736 batch_id=89:  19%|█▉        | 88/469 [00:04<00:16, 22.86it/s]\u001b[A\n",
            "loss=0.3624129593372345 batch_id=90:  19%|█▉        | 88/469 [00:04<00:16, 22.86it/s]\u001b[A\n",
            "loss=0.3624129593372345 batch_id=90:  19%|█▉        | 91/469 [00:04<00:16, 22.62it/s]\u001b[A\n",
            "loss=0.2699803411960602 batch_id=91:  19%|█▉        | 91/469 [00:04<00:16, 22.62it/s]\u001b[A\n",
            "loss=0.25295162200927734 batch_id=92:  19%|█▉        | 91/469 [00:04<00:16, 22.62it/s]\u001b[A\n",
            "loss=0.31229448318481445 batch_id=93:  19%|█▉        | 91/469 [00:04<00:16, 22.62it/s]\u001b[A\n",
            "loss=0.31229448318481445 batch_id=93:  20%|██        | 94/469 [00:04<00:17, 21.65it/s]\u001b[A\n",
            "loss=0.3048587143421173 batch_id=94:  20%|██        | 94/469 [00:04<00:17, 21.65it/s] \u001b[A\n",
            "loss=0.2501969635486603 batch_id=95:  20%|██        | 94/469 [00:04<00:17, 21.65it/s]\u001b[A\n",
            "loss=0.36399152874946594 batch_id=96:  20%|██        | 94/469 [00:04<00:17, 21.65it/s]\u001b[A\n",
            "loss=0.36399152874946594 batch_id=96:  21%|██        | 97/469 [00:04<00:17, 21.82it/s]\u001b[A\n",
            "loss=0.3040746748447418 batch_id=97:  21%|██        | 97/469 [00:04<00:17, 21.82it/s] \u001b[A\n",
            "loss=0.21731974184513092 batch_id=98:  21%|██        | 97/469 [00:04<00:17, 21.82it/s]\u001b[A\n",
            "loss=0.2333487868309021 batch_id=99:  21%|██        | 97/469 [00:04<00:17, 21.82it/s] \u001b[A\n",
            "loss=0.2333487868309021 batch_id=99:  21%|██▏       | 100/469 [00:04<00:17, 21.59it/s]\u001b[A\n",
            "loss=0.25041434168815613 batch_id=100:  21%|██▏       | 100/469 [00:04<00:17, 21.59it/s]\u001b[A\n",
            "loss=0.26245278120040894 batch_id=101:  21%|██▏       | 100/469 [00:04<00:17, 21.59it/s]\u001b[A\n",
            "loss=0.27560919523239136 batch_id=102:  21%|██▏       | 100/469 [00:04<00:17, 21.59it/s]\u001b[A\n",
            "loss=0.27560919523239136 batch_id=102:  22%|██▏       | 103/469 [00:04<00:17, 21.40it/s]\u001b[A\n",
            "loss=0.2558762729167938 batch_id=103:  22%|██▏       | 103/469 [00:04<00:17, 21.40it/s] \u001b[A\n",
            "loss=0.2966333329677582 batch_id=104:  22%|██▏       | 103/469 [00:04<00:17, 21.40it/s]\u001b[A\n",
            "loss=0.29626041650772095 batch_id=105:  22%|██▏       | 103/469 [00:04<00:17, 21.40it/s]\u001b[A\n",
            "loss=0.29626041650772095 batch_id=105:  23%|██▎       | 106/469 [00:04<00:16, 21.89it/s]\u001b[A\n",
            "loss=0.3627721071243286 batch_id=106:  23%|██▎       | 106/469 [00:05<00:16, 21.89it/s] \u001b[A\n",
            "loss=0.3612469434738159 batch_id=107:  23%|██▎       | 106/469 [00:05<00:16, 21.89it/s]\u001b[A\n",
            "loss=0.2078167051076889 batch_id=108:  23%|██▎       | 106/469 [00:05<00:16, 21.89it/s]\u001b[A\n",
            "loss=0.2078167051076889 batch_id=108:  23%|██▎       | 109/469 [00:05<00:16, 22.00it/s]\u001b[A\n",
            "loss=0.169336199760437 batch_id=109:  23%|██▎       | 109/469 [00:05<00:16, 22.00it/s] \u001b[A\n",
            "loss=0.186723992228508 batch_id=110:  23%|██▎       | 109/469 [00:05<00:16, 22.00it/s]\u001b[A\n",
            "loss=0.15688903629779816 batch_id=111:  23%|██▎       | 109/469 [00:05<00:16, 22.00it/s]\u001b[A\n",
            "loss=0.15688903629779816 batch_id=111:  24%|██▍       | 112/469 [00:05<00:16, 21.62it/s]\u001b[A\n",
            "loss=0.2214539349079132 batch_id=112:  24%|██▍       | 112/469 [00:05<00:16, 21.62it/s] \u001b[A\n",
            "loss=0.124193474650383 batch_id=113:  24%|██▍       | 112/469 [00:05<00:16, 21.62it/s] \u001b[A\n",
            "loss=0.34131407737731934 batch_id=114:  24%|██▍       | 112/469 [00:05<00:16, 21.62it/s]\u001b[A\n",
            "loss=0.34131407737731934 batch_id=114:  25%|██▍       | 115/469 [00:05<00:15, 22.23it/s]\u001b[A\n",
            "loss=0.1651535928249359 batch_id=115:  25%|██▍       | 115/469 [00:05<00:15, 22.23it/s] \u001b[A\n",
            "loss=0.1832355558872223 batch_id=116:  25%|██▍       | 115/469 [00:05<00:15, 22.23it/s]\u001b[A\n",
            "loss=0.14881908893585205 batch_id=117:  25%|██▍       | 115/469 [00:05<00:15, 22.23it/s]\u001b[A\n",
            "loss=0.14881908893585205 batch_id=117:  25%|██▌       | 118/469 [00:05<00:15, 22.06it/s]\u001b[A\n",
            "loss=0.15418460965156555 batch_id=118:  25%|██▌       | 118/469 [00:05<00:15, 22.06it/s]\u001b[A\n",
            "loss=0.1744391918182373 batch_id=119:  25%|██▌       | 118/469 [00:05<00:15, 22.06it/s] \u001b[A\n",
            "loss=0.15894317626953125 batch_id=120:  25%|██▌       | 118/469 [00:05<00:15, 22.06it/s]\u001b[A\n",
            "loss=0.15894317626953125 batch_id=120:  26%|██▌       | 121/469 [00:05<00:15, 22.21it/s]\u001b[A\n",
            "loss=0.16861999034881592 batch_id=121:  26%|██▌       | 121/469 [00:05<00:15, 22.21it/s]\u001b[A\n",
            "loss=0.20579370856285095 batch_id=122:  26%|██▌       | 121/469 [00:05<00:15, 22.21it/s]\u001b[A\n",
            "loss=0.12616945803165436 batch_id=123:  26%|██▌       | 121/469 [00:05<00:15, 22.21it/s]\u001b[A\n",
            "loss=0.12616945803165436 batch_id=123:  26%|██▋       | 124/469 [00:05<00:15, 21.73it/s]\u001b[A\n",
            "loss=0.33509472012519836 batch_id=124:  26%|██▋       | 124/469 [00:05<00:15, 21.73it/s]\u001b[A\n",
            "loss=0.1650356948375702 batch_id=125:  26%|██▋       | 124/469 [00:05<00:15, 21.73it/s] \u001b[A\n",
            "loss=0.25675442814826965 batch_id=126:  26%|██▋       | 124/469 [00:05<00:15, 21.73it/s]\u001b[A\n",
            "loss=0.25675442814826965 batch_id=126:  27%|██▋       | 127/469 [00:05<00:15, 22.13it/s]\u001b[A\n",
            "loss=0.18606913089752197 batch_id=127:  27%|██▋       | 127/469 [00:05<00:15, 22.13it/s]\u001b[A\n",
            "loss=0.17459341883659363 batch_id=128:  27%|██▋       | 127/469 [00:06<00:15, 22.13it/s]\u001b[A\n",
            "loss=0.21716807782649994 batch_id=129:  27%|██▋       | 127/469 [00:06<00:15, 22.13it/s]\u001b[A\n",
            "loss=0.21716807782649994 batch_id=129:  28%|██▊       | 130/469 [00:06<00:15, 22.12it/s]\u001b[A\n",
            "loss=0.2773159444332123 batch_id=130:  28%|██▊       | 130/469 [00:06<00:15, 22.12it/s] \u001b[A\n",
            "loss=0.3113382160663605 batch_id=131:  28%|██▊       | 130/469 [00:06<00:15, 22.12it/s]\u001b[A\n",
            "loss=0.12449324131011963 batch_id=132:  28%|██▊       | 130/469 [00:06<00:15, 22.12it/s]\u001b[A\n",
            "loss=0.12449324131011963 batch_id=132:  28%|██▊       | 133/469 [00:06<00:14, 22.77it/s]\u001b[A\n",
            "loss=0.11342887580394745 batch_id=133:  28%|██▊       | 133/469 [00:06<00:14, 22.77it/s]\u001b[A\n",
            "loss=0.16725029051303864 batch_id=134:  28%|██▊       | 133/469 [00:06<00:14, 22.77it/s]\u001b[A\n",
            "loss=0.23887744545936584 batch_id=135:  28%|██▊       | 133/469 [00:06<00:14, 22.77it/s]\u001b[A\n",
            "loss=0.23887744545936584 batch_id=135:  29%|██▉       | 136/469 [00:06<00:14, 22.51it/s]\u001b[A\n",
            "loss=0.15497292578220367 batch_id=136:  29%|██▉       | 136/469 [00:06<00:14, 22.51it/s]\u001b[A\n",
            "loss=0.2601431608200073 batch_id=137:  29%|██▉       | 136/469 [00:06<00:14, 22.51it/s] \u001b[A\n",
            "loss=0.18177568912506104 batch_id=138:  29%|██▉       | 136/469 [00:06<00:14, 22.51it/s]\u001b[A\n",
            "loss=0.18177568912506104 batch_id=138:  30%|██▉       | 139/469 [00:06<00:14, 22.35it/s]\u001b[A\n",
            "loss=0.1994573026895523 batch_id=139:  30%|██▉       | 139/469 [00:06<00:14, 22.35it/s] \u001b[A\n",
            "loss=0.17992091178894043 batch_id=140:  30%|██▉       | 139/469 [00:06<00:14, 22.35it/s]\u001b[A\n",
            "loss=0.12670353055000305 batch_id=141:  30%|██▉       | 139/469 [00:06<00:14, 22.35it/s]\u001b[A\n",
            "loss=0.12670353055000305 batch_id=141:  30%|███       | 142/469 [00:06<00:14, 22.09it/s]\u001b[A\n",
            "loss=0.1992238163948059 batch_id=142:  30%|███       | 142/469 [00:06<00:14, 22.09it/s] \u001b[A\n",
            "loss=0.13224370777606964 batch_id=143:  30%|███       | 142/469 [00:06<00:14, 22.09it/s]\u001b[A\n",
            "loss=0.13762500882148743 batch_id=144:  30%|███       | 142/469 [00:06<00:14, 22.09it/s]\u001b[A\n",
            "loss=0.13762500882148743 batch_id=144:  31%|███       | 145/469 [00:06<00:15, 21.23it/s]\u001b[A\n",
            "loss=0.15831317007541656 batch_id=145:  31%|███       | 145/469 [00:06<00:15, 21.23it/s]\u001b[A\n",
            "loss=0.18061839044094086 batch_id=146:  31%|███       | 145/469 [00:06<00:15, 21.23it/s]\u001b[A\n",
            "loss=0.18307697772979736 batch_id=147:  31%|███       | 145/469 [00:06<00:15, 21.23it/s]\u001b[A\n",
            "loss=0.18307697772979736 batch_id=147:  32%|███▏      | 148/469 [00:06<00:14, 22.21it/s]\u001b[A\n",
            "loss=0.20221473276615143 batch_id=148:  32%|███▏      | 148/469 [00:06<00:14, 22.21it/s]\u001b[A\n",
            "loss=0.11199121177196503 batch_id=149:  32%|███▏      | 148/469 [00:06<00:14, 22.21it/s]\u001b[A\n",
            "loss=0.15709573030471802 batch_id=150:  32%|███▏      | 148/469 [00:07<00:14, 22.21it/s]\u001b[A\n",
            "loss=0.15709573030471802 batch_id=150:  32%|███▏      | 151/469 [00:07<00:14, 21.52it/s]\u001b[A\n",
            "loss=0.08771234005689621 batch_id=151:  32%|███▏      | 151/469 [00:07<00:14, 21.52it/s]\u001b[A\n",
            "loss=0.12521788477897644 batch_id=152:  32%|███▏      | 151/469 [00:07<00:14, 21.52it/s]\u001b[A\n",
            "loss=0.12110529839992523 batch_id=153:  32%|███▏      | 151/469 [00:07<00:14, 21.52it/s]\u001b[A\n",
            "loss=0.12110529839992523 batch_id=153:  33%|███▎      | 154/469 [00:07<00:14, 21.99it/s]\u001b[A\n",
            "loss=0.11059350520372391 batch_id=154:  33%|███▎      | 154/469 [00:07<00:14, 21.99it/s]\u001b[A\n",
            "loss=0.24778854846954346 batch_id=155:  33%|███▎      | 154/469 [00:07<00:14, 21.99it/s]\u001b[A\n",
            "loss=0.13408534228801727 batch_id=156:  33%|███▎      | 154/469 [00:07<00:14, 21.99it/s]\u001b[A\n",
            "loss=0.13408534228801727 batch_id=156:  33%|███▎      | 157/469 [00:07<00:14, 21.69it/s]\u001b[A\n",
            "loss=0.1410854160785675 batch_id=157:  33%|███▎      | 157/469 [00:07<00:14, 21.69it/s] \u001b[A\n",
            "loss=0.14914296567440033 batch_id=158:  33%|███▎      | 157/469 [00:07<00:14, 21.69it/s]\u001b[A\n",
            "loss=0.156355619430542 batch_id=159:  33%|███▎      | 157/469 [00:07<00:14, 21.69it/s]  \u001b[A\n",
            "loss=0.156355619430542 batch_id=159:  34%|███▍      | 160/469 [00:07<00:14, 21.99it/s]\u001b[A\n",
            "loss=0.06293430924415588 batch_id=160:  34%|███▍      | 160/469 [00:07<00:14, 21.99it/s]\u001b[A\n",
            "loss=0.1515583097934723 batch_id=161:  34%|███▍      | 160/469 [00:07<00:14, 21.99it/s] \u001b[A\n",
            "loss=0.17922423779964447 batch_id=162:  34%|███▍      | 160/469 [00:07<00:14, 21.99it/s]\u001b[A\n",
            "loss=0.17922423779964447 batch_id=162:  35%|███▍      | 163/469 [00:07<00:13, 22.80it/s]\u001b[A\n",
            "loss=0.08904942125082016 batch_id=163:  35%|███▍      | 163/469 [00:07<00:13, 22.80it/s]\u001b[A\n",
            "loss=0.14840596914291382 batch_id=164:  35%|███▍      | 163/469 [00:07<00:13, 22.80it/s]\u001b[A\n",
            "loss=0.10261818766593933 batch_id=165:  35%|███▍      | 163/469 [00:07<00:13, 22.80it/s]\u001b[A\n",
            "loss=0.10261818766593933 batch_id=165:  35%|███▌      | 166/469 [00:07<00:13, 22.18it/s]\u001b[A\n",
            "loss=0.12786588072776794 batch_id=166:  35%|███▌      | 166/469 [00:07<00:13, 22.18it/s]\u001b[A\n",
            "loss=0.12839868664741516 batch_id=167:  35%|███▌      | 166/469 [00:07<00:13, 22.18it/s]\u001b[A\n",
            "loss=0.18593668937683105 batch_id=168:  35%|███▌      | 166/469 [00:07<00:13, 22.18it/s]\u001b[A\n",
            "loss=0.18593668937683105 batch_id=168:  36%|███▌      | 169/469 [00:07<00:14, 20.94it/s]\u001b[A\n",
            "loss=0.13234829902648926 batch_id=169:  36%|███▌      | 169/469 [00:07<00:14, 20.94it/s]\u001b[A\n",
            "loss=0.08604882657527924 batch_id=170:  36%|███▌      | 169/469 [00:07<00:14, 20.94it/s]\u001b[A\n",
            "loss=0.1642785221338272 batch_id=171:  36%|███▌      | 169/469 [00:07<00:14, 20.94it/s] \u001b[A\n",
            "loss=0.1642785221338272 batch_id=171:  37%|███▋      | 172/469 [00:07<00:14, 21.17it/s]\u001b[A\n",
            "loss=0.09294746816158295 batch_id=172:  37%|███▋      | 172/469 [00:08<00:14, 21.17it/s]\u001b[A\n",
            "loss=0.1114828959107399 batch_id=173:  37%|███▋      | 172/469 [00:08<00:14, 21.17it/s] \u001b[A\n",
            "loss=0.2271423488855362 batch_id=174:  37%|███▋      | 172/469 [00:08<00:14, 21.17it/s]\u001b[A\n",
            "loss=0.2271423488855362 batch_id=174:  37%|███▋      | 175/469 [00:08<00:13, 21.90it/s]\u001b[A\n",
            "loss=0.21336865425109863 batch_id=175:  37%|███▋      | 175/469 [00:08<00:13, 21.90it/s]\u001b[A\n",
            "loss=0.1405974179506302 batch_id=176:  37%|███▋      | 175/469 [00:08<00:13, 21.90it/s] \u001b[A\n",
            "loss=0.0668906718492508 batch_id=177:  37%|███▋      | 175/469 [00:08<00:13, 21.90it/s]\u001b[A\n",
            "loss=0.0668906718492508 batch_id=177:  38%|███▊      | 178/469 [00:08<00:13, 21.68it/s]\u001b[A\n",
            "loss=0.10601802170276642 batch_id=178:  38%|███▊      | 178/469 [00:08<00:13, 21.68it/s]\u001b[A\n",
            "loss=0.28429490327835083 batch_id=179:  38%|███▊      | 178/469 [00:08<00:13, 21.68it/s]\u001b[A\n",
            "loss=0.06614112854003906 batch_id=180:  38%|███▊      | 178/469 [00:08<00:13, 21.68it/s]\u001b[A\n",
            "loss=0.06614112854003906 batch_id=180:  39%|███▊      | 181/469 [00:08<00:13, 22.06it/s]\u001b[A\n",
            "loss=0.19353176653385162 batch_id=181:  39%|███▊      | 181/469 [00:08<00:13, 22.06it/s]\u001b[A\n",
            "loss=0.1817900538444519 batch_id=182:  39%|███▊      | 181/469 [00:08<00:13, 22.06it/s] \u001b[A\n",
            "loss=0.06933905929327011 batch_id=183:  39%|███▊      | 181/469 [00:08<00:13, 22.06it/s]\u001b[A\n",
            "loss=0.06933905929327011 batch_id=183:  39%|███▉      | 184/469 [00:08<00:12, 22.19it/s]\u001b[A\n",
            "loss=0.09000462293624878 batch_id=184:  39%|███▉      | 184/469 [00:08<00:12, 22.19it/s]\u001b[A\n",
            "loss=0.12357835471630096 batch_id=185:  39%|███▉      | 184/469 [00:08<00:12, 22.19it/s]\u001b[A\n",
            "loss=0.17231974005699158 batch_id=186:  39%|███▉      | 184/469 [00:08<00:12, 22.19it/s]\u001b[A\n",
            "loss=0.17231974005699158 batch_id=186:  40%|███▉      | 187/469 [00:08<00:13, 21.67it/s]\u001b[A\n",
            "loss=0.07839250564575195 batch_id=187:  40%|███▉      | 187/469 [00:08<00:13, 21.67it/s]\u001b[A\n",
            "loss=0.17829129099845886 batch_id=188:  40%|███▉      | 187/469 [00:08<00:13, 21.67it/s]\u001b[A\n",
            "loss=0.10103829205036163 batch_id=189:  40%|███▉      | 187/469 [00:08<00:13, 21.67it/s]\u001b[A\n",
            "loss=0.10103829205036163 batch_id=189:  41%|████      | 190/469 [00:08<00:12, 22.00it/s]\u001b[A\n",
            "loss=0.11174800992012024 batch_id=190:  41%|████      | 190/469 [00:08<00:12, 22.00it/s]\u001b[A\n",
            "loss=0.057604651898145676 batch_id=191:  41%|████      | 190/469 [00:08<00:12, 22.00it/s]\u001b[A\n",
            "loss=0.11118842661380768 batch_id=192:  41%|████      | 190/469 [00:08<00:12, 22.00it/s] \u001b[A\n",
            "loss=0.11118842661380768 batch_id=192:  41%|████      | 193/469 [00:08<00:12, 22.17it/s]\u001b[A\n",
            "loss=0.12702688574790955 batch_id=193:  41%|████      | 193/469 [00:08<00:12, 22.17it/s]\u001b[A\n",
            "loss=0.08719748258590698 batch_id=194:  41%|████      | 193/469 [00:09<00:12, 22.17it/s]\u001b[A\n",
            "loss=0.08630792051553726 batch_id=195:  41%|████      | 193/469 [00:09<00:12, 22.17it/s]\u001b[A\n",
            "loss=0.08630792051553726 batch_id=195:  42%|████▏     | 196/469 [00:09<00:12, 21.51it/s]\u001b[A\n",
            "loss=0.19473549723625183 batch_id=196:  42%|████▏     | 196/469 [00:09<00:12, 21.51it/s]\u001b[A\n",
            "loss=0.048306312412023544 batch_id=197:  42%|████▏     | 196/469 [00:09<00:12, 21.51it/s]\u001b[A\n",
            "loss=0.17271733283996582 batch_id=198:  42%|████▏     | 196/469 [00:09<00:12, 21.51it/s] \u001b[A\n",
            "loss=0.17271733283996582 batch_id=198:  42%|████▏     | 199/469 [00:09<00:12, 21.60it/s]\u001b[A\n",
            "loss=0.07571815699338913 batch_id=199:  42%|████▏     | 199/469 [00:09<00:12, 21.60it/s]\u001b[A\n",
            "loss=0.14969982206821442 batch_id=200:  42%|████▏     | 199/469 [00:09<00:12, 21.60it/s]\u001b[A\n",
            "loss=0.09274353086948395 batch_id=201:  42%|████▏     | 199/469 [00:09<00:12, 21.60it/s]\u001b[A\n",
            "loss=0.09274353086948395 batch_id=201:  43%|████▎     | 202/469 [00:09<00:12, 21.92it/s]\u001b[A\n",
            "loss=0.14919742941856384 batch_id=202:  43%|████▎     | 202/469 [00:09<00:12, 21.92it/s]\u001b[A\n",
            "loss=0.16926032304763794 batch_id=203:  43%|████▎     | 202/469 [00:09<00:12, 21.92it/s]\u001b[A\n",
            "loss=0.10626956820487976 batch_id=204:  43%|████▎     | 202/469 [00:09<00:12, 21.92it/s]\u001b[A\n",
            "loss=0.10626956820487976 batch_id=204:  44%|████▎     | 205/469 [00:09<00:12, 21.89it/s]\u001b[A\n",
            "loss=0.1552923619747162 batch_id=205:  44%|████▎     | 205/469 [00:09<00:12, 21.89it/s] \u001b[A\n",
            "loss=0.13952510058879852 batch_id=206:  44%|████▎     | 205/469 [00:09<00:12, 21.89it/s]\u001b[A\n",
            "loss=0.0895075798034668 batch_id=207:  44%|████▎     | 205/469 [00:09<00:12, 21.89it/s] \u001b[A\n",
            "loss=0.0895075798034668 batch_id=207:  44%|████▍     | 208/469 [00:09<00:11, 22.32it/s]\u001b[A\n",
            "loss=0.1258048117160797 batch_id=208:  44%|████▍     | 208/469 [00:09<00:11, 22.32it/s]\u001b[A\n",
            "loss=0.11229334771633148 batch_id=209:  44%|████▍     | 208/469 [00:09<00:11, 22.32it/s]\u001b[A\n",
            "loss=0.07064986228942871 batch_id=210:  44%|████▍     | 208/469 [00:09<00:11, 22.32it/s]\u001b[A\n",
            "loss=0.07064986228942871 batch_id=210:  45%|████▍     | 211/469 [00:09<00:11, 22.41it/s]\u001b[A\n",
            "loss=0.10449007153511047 batch_id=211:  45%|████▍     | 211/469 [00:09<00:11, 22.41it/s]\u001b[A\n",
            "loss=0.08295632153749466 batch_id=212:  45%|████▍     | 211/469 [00:09<00:11, 22.41it/s]\u001b[A\n",
            "loss=0.17474883794784546 batch_id=213:  45%|████▍     | 211/469 [00:09<00:11, 22.41it/s]\u001b[A\n",
            "loss=0.17474883794784546 batch_id=213:  46%|████▌     | 214/469 [00:09<00:11, 21.90it/s]\u001b[A\n",
            "loss=0.1060192659497261 batch_id=214:  46%|████▌     | 214/469 [00:09<00:11, 21.90it/s] \u001b[A\n",
            "loss=0.05968575179576874 batch_id=215:  46%|████▌     | 214/469 [00:09<00:11, 21.90it/s]\u001b[A\n",
            "loss=0.17405283451080322 batch_id=216:  46%|████▌     | 214/469 [00:10<00:11, 21.90it/s]\u001b[A\n",
            "loss=0.17405283451080322 batch_id=216:  46%|████▋     | 217/469 [00:10<00:11, 21.83it/s]\u001b[A\n",
            "loss=0.18212631344795227 batch_id=217:  46%|████▋     | 217/469 [00:10<00:11, 21.83it/s]\u001b[A\n",
            "loss=0.06537501513957977 batch_id=218:  46%|████▋     | 217/469 [00:10<00:11, 21.83it/s]\u001b[A\n",
            "loss=0.14632707834243774 batch_id=219:  46%|████▋     | 217/469 [00:10<00:11, 21.83it/s]\u001b[A\n",
            "loss=0.14632707834243774 batch_id=219:  47%|████▋     | 220/469 [00:10<00:11, 22.09it/s]\u001b[A\n",
            "loss=0.07545337080955505 batch_id=220:  47%|████▋     | 220/469 [00:10<00:11, 22.09it/s]\u001b[A\n",
            "loss=0.07141128182411194 batch_id=221:  47%|████▋     | 220/469 [00:10<00:11, 22.09it/s]\u001b[A\n",
            "loss=0.17622044682502747 batch_id=222:  47%|████▋     | 220/469 [00:10<00:11, 22.09it/s]\u001b[A\n",
            "loss=0.17622044682502747 batch_id=222:  48%|████▊     | 223/469 [00:10<00:11, 22.20it/s]\u001b[A\n",
            "loss=0.17964188754558563 batch_id=223:  48%|████▊     | 223/469 [00:10<00:11, 22.20it/s]\u001b[A\n",
            "loss=0.1239672601222992 batch_id=224:  48%|████▊     | 223/469 [00:10<00:11, 22.20it/s] \u001b[A\n",
            "loss=0.08188587427139282 batch_id=225:  48%|████▊     | 223/469 [00:10<00:11, 22.20it/s]\u001b[A\n",
            "loss=0.08188587427139282 batch_id=225:  48%|████▊     | 226/469 [00:10<00:11, 22.07it/s]\u001b[A\n",
            "loss=0.11845782399177551 batch_id=226:  48%|████▊     | 226/469 [00:10<00:11, 22.07it/s]\u001b[A\n",
            "loss=0.10875485837459564 batch_id=227:  48%|████▊     | 226/469 [00:10<00:11, 22.07it/s]\u001b[A\n",
            "loss=0.10912030935287476 batch_id=228:  48%|████▊     | 226/469 [00:10<00:11, 22.07it/s]\u001b[A\n",
            "loss=0.10912030935287476 batch_id=228:  49%|████▉     | 229/469 [00:10<00:10, 22.64it/s]\u001b[A\n",
            "loss=0.09593673050403595 batch_id=229:  49%|████▉     | 229/469 [00:10<00:10, 22.64it/s]\u001b[A\n",
            "loss=0.09741540253162384 batch_id=230:  49%|████▉     | 229/469 [00:10<00:10, 22.64it/s]\u001b[A\n",
            "loss=0.08668914437294006 batch_id=231:  49%|████▉     | 229/469 [00:10<00:10, 22.64it/s]\u001b[A\n",
            "loss=0.08668914437294006 batch_id=231:  49%|████▉     | 232/469 [00:10<00:10, 22.95it/s]\u001b[A\n",
            "loss=0.06458796560764313 batch_id=232:  49%|████▉     | 232/469 [00:10<00:10, 22.95it/s]\u001b[A\n",
            "loss=0.14837296307086945 batch_id=233:  49%|████▉     | 232/469 [00:10<00:10, 22.95it/s]\u001b[A\n",
            "loss=0.055436376482248306 batch_id=234:  49%|████▉     | 232/469 [00:10<00:10, 22.95it/s]\u001b[A\n",
            "loss=0.055436376482248306 batch_id=234:  50%|█████     | 235/469 [00:10<00:10, 22.67it/s]\u001b[A\n",
            "loss=0.13449996709823608 batch_id=235:  50%|█████     | 235/469 [00:10<00:10, 22.67it/s] \u001b[A\n",
            "loss=0.09184610843658447 batch_id=236:  50%|█████     | 235/469 [00:10<00:10, 22.67it/s]\u001b[A\n",
            "loss=0.10525427758693695 batch_id=237:  50%|█████     | 235/469 [00:10<00:10, 22.67it/s]\u001b[A\n",
            "loss=0.10525427758693695 batch_id=237:  51%|█████     | 238/469 [00:10<00:10, 22.43it/s]\u001b[A\n",
            "loss=0.05922682583332062 batch_id=238:  51%|█████     | 238/469 [00:10<00:10, 22.43it/s]\u001b[A\n",
            "loss=0.04055964946746826 batch_id=239:  51%|█████     | 238/469 [00:11<00:10, 22.43it/s]\u001b[A\n",
            "loss=0.07825721800327301 batch_id=240:  51%|█████     | 238/469 [00:11<00:10, 22.43it/s]\u001b[A\n",
            "loss=0.07825721800327301 batch_id=240:  51%|█████▏    | 241/469 [00:11<00:10, 22.47it/s]\u001b[A\n",
            "loss=0.09896169602870941 batch_id=241:  51%|█████▏    | 241/469 [00:11<00:10, 22.47it/s]\u001b[A\n",
            "loss=0.10925177484750748 batch_id=242:  51%|█████▏    | 241/469 [00:11<00:10, 22.47it/s]\u001b[A\n",
            "loss=0.16799810528755188 batch_id=243:  51%|█████▏    | 241/469 [00:11<00:10, 22.47it/s]\u001b[A\n",
            "loss=0.16799810528755188 batch_id=243:  52%|█████▏    | 244/469 [00:11<00:10, 22.32it/s]\u001b[A\n",
            "loss=0.08957003057003021 batch_id=244:  52%|█████▏    | 244/469 [00:11<00:10, 22.32it/s]\u001b[A\n",
            "loss=0.12564092874526978 batch_id=245:  52%|█████▏    | 244/469 [00:11<00:10, 22.32it/s]\u001b[A\n",
            "loss=0.03487765043973923 batch_id=246:  52%|█████▏    | 244/469 [00:11<00:10, 22.32it/s]\u001b[A\n",
            "loss=0.03487765043973923 batch_id=246:  53%|█████▎    | 247/469 [00:11<00:09, 22.31it/s]\u001b[A\n",
            "loss=0.20244541764259338 batch_id=247:  53%|█████▎    | 247/469 [00:11<00:09, 22.31it/s]\u001b[A\n",
            "loss=0.21441107988357544 batch_id=248:  53%|█████▎    | 247/469 [00:11<00:09, 22.31it/s]\u001b[A\n",
            "loss=0.059750501066446304 batch_id=249:  53%|█████▎    | 247/469 [00:11<00:09, 22.31it/s]\u001b[A\n",
            "loss=0.059750501066446304 batch_id=249:  53%|█████▎    | 250/469 [00:11<00:09, 22.79it/s]\u001b[A\n",
            "loss=0.11441797018051147 batch_id=250:  53%|█████▎    | 250/469 [00:11<00:09, 22.79it/s] \u001b[A\n",
            "loss=0.0828607827425003 batch_id=251:  53%|█████▎    | 250/469 [00:11<00:09, 22.79it/s] \u001b[A\n",
            "loss=0.028778105974197388 batch_id=252:  53%|█████▎    | 250/469 [00:11<00:09, 22.79it/s]\u001b[A\n",
            "loss=0.028778105974197388 batch_id=252:  54%|█████▍    | 253/469 [00:11<00:09, 23.06it/s]\u001b[A\n",
            "loss=0.090659499168396 batch_id=253:  54%|█████▍    | 253/469 [00:11<00:09, 23.06it/s]   \u001b[A\n",
            "loss=0.05481431260704994 batch_id=254:  54%|█████▍    | 253/469 [00:11<00:09, 23.06it/s]\u001b[A\n",
            "loss=0.10953401029109955 batch_id=255:  54%|█████▍    | 253/469 [00:11<00:09, 23.06it/s]\u001b[A\n",
            "loss=0.10953401029109955 batch_id=255:  55%|█████▍    | 256/469 [00:11<00:09, 22.53it/s]\u001b[A\n",
            "loss=0.23826879262924194 batch_id=256:  55%|█████▍    | 256/469 [00:11<00:09, 22.53it/s]\u001b[A\n",
            "loss=0.08736931532621384 batch_id=257:  55%|█████▍    | 256/469 [00:11<00:09, 22.53it/s]\u001b[A\n",
            "loss=0.18837733566761017 batch_id=258:  55%|█████▍    | 256/469 [00:11<00:09, 22.53it/s]\u001b[A\n",
            "loss=0.18837733566761017 batch_id=258:  55%|█████▌    | 259/469 [00:11<00:09, 21.56it/s]\u001b[A\n",
            "loss=0.06485839933156967 batch_id=259:  55%|█████▌    | 259/469 [00:11<00:09, 21.56it/s]\u001b[A\n",
            "loss=0.036108970642089844 batch_id=260:  55%|█████▌    | 259/469 [00:11<00:09, 21.56it/s]\u001b[A\n",
            "loss=0.0636402815580368 batch_id=261:  55%|█████▌    | 259/469 [00:12<00:09, 21.56it/s]  \u001b[A\n",
            "loss=0.0636402815580368 batch_id=261:  56%|█████▌    | 262/469 [00:12<00:09, 22.17it/s]\u001b[A\n",
            "loss=0.13122960925102234 batch_id=262:  56%|█████▌    | 262/469 [00:12<00:09, 22.17it/s]\u001b[A\n",
            "loss=0.10395212471485138 batch_id=263:  56%|█████▌    | 262/469 [00:12<00:09, 22.17it/s]\u001b[A\n",
            "loss=0.1724826544523239 batch_id=264:  56%|█████▌    | 262/469 [00:12<00:09, 22.17it/s] \u001b[A\n",
            "loss=0.1724826544523239 batch_id=264:  57%|█████▋    | 265/469 [00:12<00:09, 22.30it/s]\u001b[A\n",
            "loss=0.12845568358898163 batch_id=265:  57%|█████▋    | 265/469 [00:12<00:09, 22.30it/s]\u001b[A\n",
            "loss=0.10557319223880768 batch_id=266:  57%|█████▋    | 265/469 [00:12<00:09, 22.30it/s]\u001b[A\n",
            "loss=0.1403321772813797 batch_id=267:  57%|█████▋    | 265/469 [00:12<00:09, 22.30it/s] \u001b[A\n",
            "loss=0.1403321772813797 batch_id=267:  57%|█████▋    | 268/469 [00:12<00:09, 22.07it/s]\u001b[A\n",
            "loss=0.08907969295978546 batch_id=268:  57%|█████▋    | 268/469 [00:12<00:09, 22.07it/s]\u001b[A\n",
            "loss=0.06857930123806 batch_id=269:  57%|█████▋    | 268/469 [00:12<00:09, 22.07it/s]   \u001b[A\n",
            "loss=0.04879488795995712 batch_id=270:  57%|█████▋    | 268/469 [00:12<00:09, 22.07it/s]\u001b[A\n",
            "loss=0.04879488795995712 batch_id=270:  58%|█████▊    | 271/469 [00:12<00:08, 22.08it/s]\u001b[A\n",
            "loss=0.09770037233829498 batch_id=271:  58%|█████▊    | 271/469 [00:12<00:08, 22.08it/s]\u001b[A\n",
            "loss=0.08723706752061844 batch_id=272:  58%|█████▊    | 271/469 [00:12<00:08, 22.08it/s]\u001b[A\n",
            "loss=0.09946379065513611 batch_id=273:  58%|█████▊    | 271/469 [00:12<00:08, 22.08it/s]\u001b[A\n",
            "loss=0.09946379065513611 batch_id=273:  58%|█████▊    | 274/469 [00:12<00:08, 22.57it/s]\u001b[A\n",
            "loss=0.08742202818393707 batch_id=274:  58%|█████▊    | 274/469 [00:12<00:08, 22.57it/s]\u001b[A\n",
            "loss=0.17812705039978027 batch_id=275:  58%|█████▊    | 274/469 [00:12<00:08, 22.57it/s]\u001b[A\n",
            "loss=0.0832928717136383 batch_id=276:  58%|█████▊    | 274/469 [00:12<00:08, 22.57it/s] \u001b[A\n",
            "loss=0.0832928717136383 batch_id=276:  59%|█████▉    | 277/469 [00:12<00:08, 22.51it/s]\u001b[A\n",
            "loss=0.09135499596595764 batch_id=277:  59%|█████▉    | 277/469 [00:12<00:08, 22.51it/s]\u001b[A\n",
            "loss=0.016600150614976883 batch_id=278:  59%|█████▉    | 277/469 [00:12<00:08, 22.51it/s]\u001b[A\n",
            "loss=0.056216493248939514 batch_id=279:  59%|█████▉    | 277/469 [00:12<00:08, 22.51it/s]\u001b[A\n",
            "loss=0.056216493248939514 batch_id=279:  60%|█████▉    | 280/469 [00:12<00:08, 21.86it/s]\u001b[A\n",
            "loss=0.15647968649864197 batch_id=280:  60%|█████▉    | 280/469 [00:12<00:08, 21.86it/s] \u001b[A\n",
            "loss=0.1588909924030304 batch_id=281:  60%|█████▉    | 280/469 [00:12<00:08, 21.86it/s] \u001b[A\n",
            "loss=0.13698889315128326 batch_id=282:  60%|█████▉    | 280/469 [00:12<00:08, 21.86it/s]\u001b[A\n",
            "loss=0.13698889315128326 batch_id=282:  60%|██████    | 283/469 [00:12<00:08, 21.78it/s]\u001b[A\n",
            "loss=0.11596335470676422 batch_id=283:  60%|██████    | 283/469 [00:13<00:08, 21.78it/s]\u001b[A\n",
            "loss=0.03438619151711464 batch_id=284:  60%|██████    | 283/469 [00:13<00:08, 21.78it/s]\u001b[A\n",
            "loss=0.05258724465966225 batch_id=285:  60%|██████    | 283/469 [00:13<00:08, 21.78it/s]\u001b[A\n",
            "loss=0.05258724465966225 batch_id=285:  61%|██████    | 286/469 [00:13<00:08, 22.46it/s]\u001b[A\n",
            "loss=0.10684630274772644 batch_id=286:  61%|██████    | 286/469 [00:13<00:08, 22.46it/s]\u001b[A\n",
            "loss=0.1581842005252838 batch_id=287:  61%|██████    | 286/469 [00:13<00:08, 22.46it/s] \u001b[A\n",
            "loss=0.1659010648727417 batch_id=288:  61%|██████    | 286/469 [00:13<00:08, 22.46it/s]\u001b[A\n",
            "loss=0.1659010648727417 batch_id=288:  62%|██████▏   | 289/469 [00:13<00:08, 21.72it/s]\u001b[A\n",
            "loss=0.08963842689990997 batch_id=289:  62%|██████▏   | 289/469 [00:13<00:08, 21.72it/s]\u001b[A\n",
            "loss=0.15244421362876892 batch_id=290:  62%|██████▏   | 289/469 [00:13<00:08, 21.72it/s]\u001b[A\n",
            "loss=0.07391100376844406 batch_id=291:  62%|██████▏   | 289/469 [00:13<00:08, 21.72it/s]\u001b[A\n",
            "loss=0.07391100376844406 batch_id=291:  62%|██████▏   | 292/469 [00:13<00:08, 21.76it/s]\u001b[A\n",
            "loss=0.08717652410268784 batch_id=292:  62%|██████▏   | 292/469 [00:13<00:08, 21.76it/s]\u001b[A\n",
            "loss=0.032368771731853485 batch_id=293:  62%|██████▏   | 292/469 [00:13<00:08, 21.76it/s]\u001b[A\n",
            "loss=0.1603180170059204 batch_id=294:  62%|██████▏   | 292/469 [00:13<00:08, 21.76it/s]  \u001b[A\n",
            "loss=0.1603180170059204 batch_id=294:  63%|██████▎   | 295/469 [00:13<00:07, 22.22it/s]\u001b[A\n",
            "loss=0.047380510717630386 batch_id=295:  63%|██████▎   | 295/469 [00:13<00:07, 22.22it/s]\u001b[A\n",
            "loss=0.04426712915301323 batch_id=296:  63%|██████▎   | 295/469 [00:13<00:07, 22.22it/s] \u001b[A\n",
            "loss=0.13530409336090088 batch_id=297:  63%|██████▎   | 295/469 [00:13<00:07, 22.22it/s]\u001b[A\n",
            "loss=0.13530409336090088 batch_id=297:  64%|██████▎   | 298/469 [00:13<00:07, 22.12it/s]\u001b[A\n",
            "loss=0.028730779886245728 batch_id=298:  64%|██████▎   | 298/469 [00:13<00:07, 22.12it/s]\u001b[A\n",
            "loss=0.04809645190834999 batch_id=299:  64%|██████▎   | 298/469 [00:13<00:07, 22.12it/s] \u001b[A\n",
            "loss=0.22051721811294556 batch_id=300:  64%|██████▎   | 298/469 [00:13<00:07, 22.12it/s]\u001b[A\n",
            "loss=0.22051721811294556 batch_id=300:  64%|██████▍   | 301/469 [00:13<00:07, 22.94it/s]\u001b[A\n",
            "loss=0.08809450268745422 batch_id=301:  64%|██████▍   | 301/469 [00:13<00:07, 22.94it/s]\u001b[A\n",
            "loss=0.08020834624767303 batch_id=302:  64%|██████▍   | 301/469 [00:13<00:07, 22.94it/s]\u001b[A\n",
            "loss=0.07956042140722275 batch_id=303:  64%|██████▍   | 301/469 [00:13<00:07, 22.94it/s]\u001b[A\n",
            "loss=0.07956042140722275 batch_id=303:  65%|██████▍   | 304/469 [00:13<00:07, 22.26it/s]\u001b[A\n",
            "loss=0.0949978232383728 batch_id=304:  65%|██████▍   | 304/469 [00:13<00:07, 22.26it/s] \u001b[A\n",
            "loss=0.3176959156990051 batch_id=305:  65%|██████▍   | 304/469 [00:14<00:07, 22.26it/s]\u001b[A\n",
            "loss=0.06291927397251129 batch_id=306:  65%|██████▍   | 304/469 [00:14<00:07, 22.26it/s]\u001b[A\n",
            "loss=0.06291927397251129 batch_id=306:  65%|██████▌   | 307/469 [00:14<00:07, 21.89it/s]\u001b[A\n",
            "loss=0.031490713357925415 batch_id=307:  65%|██████▌   | 307/469 [00:14<00:07, 21.89it/s]\u001b[A\n",
            "loss=0.07799957692623138 batch_id=308:  65%|██████▌   | 307/469 [00:14<00:07, 21.89it/s] \u001b[A\n",
            "loss=0.04510219022631645 batch_id=309:  65%|██████▌   | 307/469 [00:14<00:07, 21.89it/s]\u001b[A\n",
            "loss=0.04510219022631645 batch_id=309:  66%|██████▌   | 310/469 [00:14<00:07, 22.00it/s]\u001b[A\n",
            "loss=0.09850505739450455 batch_id=310:  66%|██████▌   | 310/469 [00:14<00:07, 22.00it/s]\u001b[A\n",
            "loss=0.042954474687576294 batch_id=311:  66%|██████▌   | 310/469 [00:14<00:07, 22.00it/s]\u001b[A\n",
            "loss=0.14396516978740692 batch_id=312:  66%|██████▌   | 310/469 [00:14<00:07, 22.00it/s] \u001b[A\n",
            "loss=0.14396516978740692 batch_id=312:  67%|██████▋   | 313/469 [00:14<00:07, 21.86it/s]\u001b[A\n",
            "loss=0.10491415858268738 batch_id=313:  67%|██████▋   | 313/469 [00:14<00:07, 21.86it/s]\u001b[A\n",
            "loss=0.08043418824672699 batch_id=314:  67%|██████▋   | 313/469 [00:14<00:07, 21.86it/s]\u001b[A\n",
            "loss=0.23159898817539215 batch_id=315:  67%|██████▋   | 313/469 [00:14<00:07, 21.86it/s]\u001b[A\n",
            "loss=0.23159898817539215 batch_id=315:  67%|██████▋   | 316/469 [00:14<00:06, 22.14it/s]\u001b[A\n",
            "loss=0.17465437948703766 batch_id=316:  67%|██████▋   | 316/469 [00:14<00:06, 22.14it/s]\u001b[A\n",
            "loss=0.06358733773231506 batch_id=317:  67%|██████▋   | 316/469 [00:14<00:06, 22.14it/s]\u001b[A\n",
            "loss=0.09221957623958588 batch_id=318:  67%|██████▋   | 316/469 [00:14<00:06, 22.14it/s]\u001b[A\n",
            "loss=0.09221957623958588 batch_id=318:  68%|██████▊   | 319/469 [00:14<00:06, 21.73it/s]\u001b[A\n",
            "loss=0.07555662095546722 batch_id=319:  68%|██████▊   | 319/469 [00:14<00:06, 21.73it/s]\u001b[A\n",
            "loss=0.0649232417345047 batch_id=320:  68%|██████▊   | 319/469 [00:14<00:06, 21.73it/s] \u001b[A\n",
            "loss=0.134675532579422 batch_id=321:  68%|██████▊   | 319/469 [00:14<00:06, 21.73it/s] \u001b[A\n",
            "loss=0.134675532579422 batch_id=321:  69%|██████▊   | 322/469 [00:14<00:06, 21.95it/s]\u001b[A\n",
            "loss=0.0828561931848526 batch_id=322:  69%|██████▊   | 322/469 [00:14<00:06, 21.95it/s]\u001b[A\n",
            "loss=0.049115970730781555 batch_id=323:  69%|██████▊   | 322/469 [00:14<00:06, 21.95it/s]\u001b[A\n",
            "loss=0.06027177348732948 batch_id=324:  69%|██████▊   | 322/469 [00:14<00:06, 21.95it/s] \u001b[A\n",
            "loss=0.06027177348732948 batch_id=324:  69%|██████▉   | 325/469 [00:14<00:06, 21.12it/s]\u001b[A\n",
            "loss=0.02031814306974411 batch_id=325:  69%|██████▉   | 325/469 [00:14<00:06, 21.12it/s]\u001b[A\n",
            "loss=0.04809655249118805 batch_id=326:  69%|██████▉   | 325/469 [00:14<00:06, 21.12it/s]\u001b[A\n",
            "loss=0.08620649576187134 batch_id=327:  69%|██████▉   | 325/469 [00:15<00:06, 21.12it/s]\u001b[A\n",
            "loss=0.08620649576187134 batch_id=327:  70%|██████▉   | 328/469 [00:15<00:06, 21.14it/s]\u001b[A\n",
            "loss=0.05952872335910797 batch_id=328:  70%|██████▉   | 328/469 [00:15<00:06, 21.14it/s]\u001b[A\n",
            "loss=0.17988276481628418 batch_id=329:  70%|██████▉   | 328/469 [00:15<00:06, 21.14it/s]\u001b[A\n",
            "loss=0.037608951330184937 batch_id=330:  70%|██████▉   | 328/469 [00:15<00:06, 21.14it/s]\u001b[A\n",
            "loss=0.037608951330184937 batch_id=330:  71%|███████   | 331/469 [00:15<00:06, 21.08it/s]\u001b[A\n",
            "loss=0.10865409672260284 batch_id=331:  71%|███████   | 331/469 [00:15<00:06, 21.08it/s] \u001b[A\n",
            "loss=0.05599328130483627 batch_id=332:  71%|███████   | 331/469 [00:15<00:06, 21.08it/s]\u001b[A\n",
            "loss=0.07946199178695679 batch_id=333:  71%|███████   | 331/469 [00:15<00:06, 21.08it/s]\u001b[A\n",
            "loss=0.07946199178695679 batch_id=333:  71%|███████   | 334/469 [00:15<00:06, 21.61it/s]\u001b[A\n",
            "loss=0.0435088649392128 batch_id=334:  71%|███████   | 334/469 [00:15<00:06, 21.61it/s] \u001b[A\n",
            "loss=0.07703925669193268 batch_id=335:  71%|███████   | 334/469 [00:15<00:06, 21.61it/s]\u001b[A\n",
            "loss=0.05733189359307289 batch_id=336:  71%|███████   | 334/469 [00:15<00:06, 21.61it/s]\u001b[A\n",
            "loss=0.05733189359307289 batch_id=336:  72%|███████▏  | 337/469 [00:15<00:06, 21.85it/s]\u001b[A\n",
            "loss=0.05013800412416458 batch_id=337:  72%|███████▏  | 337/469 [00:15<00:06, 21.85it/s]\u001b[A\n",
            "loss=0.05682562664151192 batch_id=338:  72%|███████▏  | 337/469 [00:15<00:06, 21.85it/s]\u001b[A\n",
            "loss=0.17489835619926453 batch_id=339:  72%|███████▏  | 337/469 [00:15<00:06, 21.85it/s]\u001b[A\n",
            "loss=0.17489835619926453 batch_id=339:  72%|███████▏  | 340/469 [00:15<00:05, 21.72it/s]\u001b[A\n",
            "loss=0.018241818994283676 batch_id=340:  72%|███████▏  | 340/469 [00:15<00:05, 21.72it/s]\u001b[A\n",
            "loss=0.06254321336746216 batch_id=341:  72%|███████▏  | 340/469 [00:15<00:05, 21.72it/s] \u001b[A\n",
            "loss=0.07353206723928452 batch_id=342:  72%|███████▏  | 340/469 [00:15<00:05, 21.72it/s]\u001b[A\n",
            "loss=0.07353206723928452 batch_id=342:  73%|███████▎  | 343/469 [00:15<00:05, 21.85it/s]\u001b[A\n",
            "loss=0.08808959275484085 batch_id=343:  73%|███████▎  | 343/469 [00:15<00:05, 21.85it/s]\u001b[A\n",
            "loss=0.05420219153165817 batch_id=344:  73%|███████▎  | 343/469 [00:15<00:05, 21.85it/s]\u001b[A\n",
            "loss=0.08360113948583603 batch_id=345:  73%|███████▎  | 343/469 [00:15<00:05, 21.85it/s]\u001b[A\n",
            "loss=0.08360113948583603 batch_id=345:  74%|███████▍  | 346/469 [00:15<00:05, 21.37it/s]\u001b[A\n",
            "loss=0.08969926089048386 batch_id=346:  74%|███████▍  | 346/469 [00:15<00:05, 21.37it/s]\u001b[A\n",
            "loss=0.053357888013124466 batch_id=347:  74%|███████▍  | 346/469 [00:15<00:05, 21.37it/s]\u001b[A\n",
            "loss=0.01249348372220993 batch_id=348:  74%|███████▍  | 346/469 [00:16<00:05, 21.37it/s] \u001b[A\n",
            "loss=0.01249348372220993 batch_id=348:  74%|███████▍  | 349/469 [00:16<00:05, 21.58it/s]\u001b[A\n",
            "loss=0.06534449756145477 batch_id=349:  74%|███████▍  | 349/469 [00:16<00:05, 21.58it/s]\u001b[A\n",
            "loss=0.037602294236421585 batch_id=350:  74%|███████▍  | 349/469 [00:16<00:05, 21.58it/s]\u001b[A\n",
            "loss=0.025585774332284927 batch_id=351:  74%|███████▍  | 349/469 [00:16<00:05, 21.58it/s]\u001b[A\n",
            "loss=0.025585774332284927 batch_id=351:  75%|███████▌  | 352/469 [00:16<00:05, 21.11it/s]\u001b[A\n",
            "loss=0.07797328382730484 batch_id=352:  75%|███████▌  | 352/469 [00:16<00:05, 21.11it/s] \u001b[A\n",
            "loss=0.057715244591236115 batch_id=353:  75%|███████▌  | 352/469 [00:16<00:05, 21.11it/s]\u001b[A\n",
            "loss=0.10692509263753891 batch_id=354:  75%|███████▌  | 352/469 [00:16<00:05, 21.11it/s] \u001b[A\n",
            "loss=0.10692509263753891 batch_id=354:  76%|███████▌  | 355/469 [00:16<00:05, 22.06it/s]\u001b[A\n",
            "loss=0.07198210060596466 batch_id=355:  76%|███████▌  | 355/469 [00:16<00:05, 22.06it/s]\u001b[A\n",
            "loss=0.08242238312959671 batch_id=356:  76%|███████▌  | 355/469 [00:16<00:05, 22.06it/s]\u001b[A\n",
            "loss=0.052559103816747665 batch_id=357:  76%|███████▌  | 355/469 [00:16<00:05, 22.06it/s]\u001b[A\n",
            "loss=0.052559103816747665 batch_id=357:  76%|███████▋  | 358/469 [00:16<00:04, 22.36it/s]\u001b[A\n",
            "loss=0.05232201889157295 batch_id=358:  76%|███████▋  | 358/469 [00:16<00:04, 22.36it/s] \u001b[A\n",
            "loss=0.04391860589385033 batch_id=359:  76%|███████▋  | 358/469 [00:16<00:04, 22.36it/s]\u001b[A\n",
            "loss=0.08891062438488007 batch_id=360:  76%|███████▋  | 358/469 [00:16<00:04, 22.36it/s]\u001b[A\n",
            "loss=0.08891062438488007 batch_id=360:  77%|███████▋  | 361/469 [00:16<00:04, 22.61it/s]\u001b[A\n",
            "loss=0.09627646207809448 batch_id=361:  77%|███████▋  | 361/469 [00:16<00:04, 22.61it/s]\u001b[A\n",
            "loss=0.028142711147665977 batch_id=362:  77%|███████▋  | 361/469 [00:16<00:04, 22.61it/s]\u001b[A\n",
            "loss=0.15045279264450073 batch_id=363:  77%|███████▋  | 361/469 [00:16<00:04, 22.61it/s] \u001b[A\n",
            "loss=0.15045279264450073 batch_id=363:  78%|███████▊  | 364/469 [00:16<00:04, 22.66it/s]\u001b[A\n",
            "loss=0.07406298816204071 batch_id=364:  78%|███████▊  | 364/469 [00:16<00:04, 22.66it/s]\u001b[A\n",
            "loss=0.060435086488723755 batch_id=365:  78%|███████▊  | 364/469 [00:16<00:04, 22.66it/s]\u001b[A\n",
            "loss=0.07638897746801376 batch_id=366:  78%|███████▊  | 364/469 [00:16<00:04, 22.66it/s] \u001b[A\n",
            "loss=0.07638897746801376 batch_id=366:  78%|███████▊  | 367/469 [00:16<00:04, 22.66it/s]\u001b[A\n",
            "loss=0.037793196737766266 batch_id=367:  78%|███████▊  | 367/469 [00:16<00:04, 22.66it/s]\u001b[A\n",
            "loss=0.11783479154109955 batch_id=368:  78%|███████▊  | 367/469 [00:16<00:04, 22.66it/s] \u001b[A\n",
            "loss=0.08588807284832001 batch_id=369:  78%|███████▊  | 367/469 [00:16<00:04, 22.66it/s]\u001b[A\n",
            "loss=0.08588807284832001 batch_id=369:  79%|███████▉  | 370/469 [00:16<00:04, 22.06it/s]\u001b[A\n",
            "loss=0.06638337671756744 batch_id=370:  79%|███████▉  | 370/469 [00:16<00:04, 22.06it/s]\u001b[A\n",
            "loss=0.09339025616645813 batch_id=371:  79%|███████▉  | 370/469 [00:17<00:04, 22.06it/s]\u001b[A\n",
            "loss=0.09761638939380646 batch_id=372:  79%|███████▉  | 370/469 [00:17<00:04, 22.06it/s]\u001b[A\n",
            "loss=0.09761638939380646 batch_id=372:  80%|███████▉  | 373/469 [00:17<00:04, 21.92it/s]\u001b[A\n",
            "loss=0.06715469062328339 batch_id=373:  80%|███████▉  | 373/469 [00:17<00:04, 21.92it/s]\u001b[A\n",
            "loss=0.03267539665102959 batch_id=374:  80%|███████▉  | 373/469 [00:17<00:04, 21.92it/s]\u001b[A\n",
            "loss=0.07168050110340118 batch_id=375:  80%|███████▉  | 373/469 [00:17<00:04, 21.92it/s]\u001b[A\n",
            "loss=0.07168050110340118 batch_id=375:  80%|████████  | 376/469 [00:17<00:04, 22.37it/s]\u001b[A\n",
            "loss=0.06897947192192078 batch_id=376:  80%|████████  | 376/469 [00:17<00:04, 22.37it/s]\u001b[A\n",
            "loss=0.07133755087852478 batch_id=377:  80%|████████  | 376/469 [00:17<00:04, 22.37it/s]\u001b[A\n",
            "loss=0.1544172167778015 batch_id=378:  80%|████████  | 376/469 [00:17<00:04, 22.37it/s] \u001b[A\n",
            "loss=0.1544172167778015 batch_id=378:  81%|████████  | 379/469 [00:17<00:04, 22.33it/s]\u001b[A\n",
            "loss=0.08773494511842728 batch_id=379:  81%|████████  | 379/469 [00:17<00:04, 22.33it/s]\u001b[A\n",
            "loss=0.09004591405391693 batch_id=380:  81%|████████  | 379/469 [00:17<00:04, 22.33it/s]\u001b[A\n",
            "loss=0.06718242913484573 batch_id=381:  81%|████████  | 379/469 [00:17<00:04, 22.33it/s]\u001b[A\n",
            "loss=0.06718242913484573 batch_id=381:  81%|████████▏ | 382/469 [00:17<00:04, 21.03it/s]\u001b[A\n",
            "loss=0.07657548040151596 batch_id=382:  81%|████████▏ | 382/469 [00:17<00:04, 21.03it/s]\u001b[A\n",
            "loss=0.08547990024089813 batch_id=383:  81%|████████▏ | 382/469 [00:17<00:04, 21.03it/s]\u001b[A\n",
            "loss=0.05295992270112038 batch_id=384:  81%|████████▏ | 382/469 [00:17<00:04, 21.03it/s]\u001b[A\n",
            "loss=0.05295992270112038 batch_id=384:  82%|████████▏ | 385/469 [00:17<00:03, 21.55it/s]\u001b[A\n",
            "loss=0.24036520719528198 batch_id=385:  82%|████████▏ | 385/469 [00:17<00:03, 21.55it/s]\u001b[A\n",
            "loss=0.012613039463758469 batch_id=386:  82%|████████▏ | 385/469 [00:17<00:03, 21.55it/s]\u001b[A\n",
            "loss=0.06460953503847122 batch_id=387:  82%|████████▏ | 385/469 [00:17<00:03, 21.55it/s] \u001b[A\n",
            "loss=0.06460953503847122 batch_id=387:  83%|████████▎ | 388/469 [00:17<00:03, 21.95it/s]\u001b[A\n",
            "loss=0.07207927852869034 batch_id=388:  83%|████████▎ | 388/469 [00:17<00:03, 21.95it/s]\u001b[A\n",
            "loss=0.212554469704628 batch_id=389:  83%|████████▎ | 388/469 [00:17<00:03, 21.95it/s]  \u001b[A\n",
            "loss=0.06148354336619377 batch_id=390:  83%|████████▎ | 388/469 [00:17<00:03, 21.95it/s]\u001b[A\n",
            "loss=0.06148354336619377 batch_id=390:  83%|████████▎ | 391/469 [00:17<00:03, 22.66it/s]\u001b[A\n",
            "loss=0.14242155849933624 batch_id=391:  83%|████████▎ | 391/469 [00:17<00:03, 22.66it/s]\u001b[A\n",
            "loss=0.04687768220901489 batch_id=392:  83%|████████▎ | 391/469 [00:17<00:03, 22.66it/s]\u001b[A\n",
            "loss=0.14491648972034454 batch_id=393:  83%|████████▎ | 391/469 [00:18<00:03, 22.66it/s]\u001b[A\n",
            "loss=0.14491648972034454 batch_id=393:  84%|████████▍ | 394/469 [00:18<00:03, 21.92it/s]\u001b[A\n",
            "loss=0.09651260823011398 batch_id=394:  84%|████████▍ | 394/469 [00:18<00:03, 21.92it/s]\u001b[A\n",
            "loss=0.0747700110077858 batch_id=395:  84%|████████▍ | 394/469 [00:18<00:03, 21.92it/s] \u001b[A\n",
            "loss=0.08345917612314224 batch_id=396:  84%|████████▍ | 394/469 [00:18<00:03, 21.92it/s]\u001b[A\n",
            "loss=0.08345917612314224 batch_id=396:  85%|████████▍ | 397/469 [00:18<00:03, 21.60it/s]\u001b[A\n",
            "loss=0.047841548919677734 batch_id=397:  85%|████████▍ | 397/469 [00:18<00:03, 21.60it/s]\u001b[A\n",
            "loss=0.08758344501256943 batch_id=398:  85%|████████▍ | 397/469 [00:18<00:03, 21.60it/s] \u001b[A\n",
            "loss=0.044462189078330994 batch_id=399:  85%|████████▍ | 397/469 [00:18<00:03, 21.60it/s]\u001b[A\n",
            "loss=0.044462189078330994 batch_id=399:  85%|████████▌ | 400/469 [00:18<00:03, 21.60it/s]\u001b[A\n",
            "loss=0.15732687711715698 batch_id=400:  85%|████████▌ | 400/469 [00:18<00:03, 21.60it/s] \u001b[A\n",
            "loss=0.04420812800526619 batch_id=401:  85%|████████▌ | 400/469 [00:18<00:03, 21.60it/s]\u001b[A\n",
            "loss=0.059714652597904205 batch_id=402:  85%|████████▌ | 400/469 [00:18<00:03, 21.60it/s]\u001b[A\n",
            "loss=0.059714652597904205 batch_id=402:  86%|████████▌ | 403/469 [00:18<00:03, 21.75it/s]\u001b[A\n",
            "loss=0.06924412399530411 batch_id=403:  86%|████████▌ | 403/469 [00:18<00:03, 21.75it/s] \u001b[A\n",
            "loss=0.08173292875289917 batch_id=404:  86%|████████▌ | 403/469 [00:18<00:03, 21.75it/s]\u001b[A\n",
            "loss=0.02832852303981781 batch_id=405:  86%|████████▌ | 403/469 [00:18<00:03, 21.75it/s]\u001b[A\n",
            "loss=0.02832852303981781 batch_id=405:  87%|████████▋ | 406/469 [00:18<00:02, 21.66it/s]\u001b[A\n",
            "loss=0.06100008264183998 batch_id=406:  87%|████████▋ | 406/469 [00:18<00:02, 21.66it/s]\u001b[A\n",
            "loss=0.0622701495885849 batch_id=407:  87%|████████▋ | 406/469 [00:18<00:02, 21.66it/s] \u001b[A\n",
            "loss=0.07090836763381958 batch_id=408:  87%|████████▋ | 406/469 [00:18<00:02, 21.66it/s]\u001b[A\n",
            "loss=0.07090836763381958 batch_id=408:  87%|████████▋ | 409/469 [00:18<00:02, 21.46it/s]\u001b[A\n",
            "loss=0.08107990771532059 batch_id=409:  87%|████████▋ | 409/469 [00:18<00:02, 21.46it/s]\u001b[A\n",
            "loss=0.08257216215133667 batch_id=410:  87%|████████▋ | 409/469 [00:18<00:02, 21.46it/s]\u001b[A\n",
            "loss=0.08073212206363678 batch_id=411:  87%|████████▋ | 409/469 [00:18<00:02, 21.46it/s]\u001b[A\n",
            "loss=0.08073212206363678 batch_id=411:  88%|████████▊ | 412/469 [00:18<00:02, 21.43it/s]\u001b[A\n",
            "loss=0.08930517733097076 batch_id=412:  88%|████████▊ | 412/469 [00:18<00:02, 21.43it/s]\u001b[A\n",
            "loss=0.03285401314496994 batch_id=413:  88%|████████▊ | 412/469 [00:18<00:02, 21.43it/s]\u001b[A\n",
            "loss=0.038801807910203934 batch_id=414:  88%|████████▊ | 412/469 [00:19<00:02, 21.43it/s]\u001b[A\n",
            "loss=0.038801807910203934 batch_id=414:  88%|████████▊ | 415/469 [00:19<00:02, 21.43it/s]\u001b[A\n",
            "loss=0.04402200132608414 batch_id=415:  88%|████████▊ | 415/469 [00:19<00:02, 21.43it/s] \u001b[A\n",
            "loss=0.03718200698494911 batch_id=416:  88%|████████▊ | 415/469 [00:19<00:02, 21.43it/s]\u001b[A\n",
            "loss=0.10568264126777649 batch_id=417:  88%|████████▊ | 415/469 [00:19<00:02, 21.43it/s]\u001b[A\n",
            "loss=0.10568264126777649 batch_id=417:  89%|████████▉ | 418/469 [00:19<00:02, 20.95it/s]\u001b[A\n",
            "loss=0.1470690816640854 batch_id=418:  89%|████████▉ | 418/469 [00:19<00:02, 20.95it/s] \u001b[A\n",
            "loss=0.04630133509635925 batch_id=419:  89%|████████▉ | 418/469 [00:19<00:02, 20.95it/s]\u001b[A\n",
            "loss=0.06617744266986847 batch_id=420:  89%|████████▉ | 418/469 [00:19<00:02, 20.95it/s]\u001b[A\n",
            "loss=0.06617744266986847 batch_id=420:  90%|████████▉ | 421/469 [00:19<00:02, 21.31it/s]\u001b[A\n",
            "loss=0.04378962516784668 batch_id=421:  90%|████████▉ | 421/469 [00:19<00:02, 21.31it/s]\u001b[A\n",
            "loss=0.059612937271595 batch_id=422:  90%|████████▉ | 421/469 [00:19<00:02, 21.31it/s]  \u001b[A\n",
            "loss=0.08189541101455688 batch_id=423:  90%|████████▉ | 421/469 [00:19<00:02, 21.31it/s]\u001b[A\n",
            "loss=0.08189541101455688 batch_id=423:  90%|█████████ | 424/469 [00:19<00:02, 21.96it/s]\u001b[A\n",
            "loss=0.0572478212416172 batch_id=424:  90%|█████████ | 424/469 [00:19<00:02, 21.96it/s] \u001b[A\n",
            "loss=0.053821250796318054 batch_id=425:  90%|█████████ | 424/469 [00:19<00:02, 21.96it/s]\u001b[A\n",
            "loss=0.08034246414899826 batch_id=426:  90%|█████████ | 424/469 [00:19<00:02, 21.96it/s] \u001b[A\n",
            "loss=0.08034246414899826 batch_id=426:  91%|█████████ | 427/469 [00:19<00:01, 22.69it/s]\u001b[A\n",
            "loss=0.053074486553668976 batch_id=427:  91%|█████████ | 427/469 [00:19<00:01, 22.69it/s]\u001b[A\n",
            "loss=0.04777121543884277 batch_id=428:  91%|█████████ | 427/469 [00:19<00:01, 22.69it/s] \u001b[A\n",
            "loss=0.04963762313127518 batch_id=429:  91%|█████████ | 427/469 [00:19<00:01, 22.69it/s]\u001b[A\n",
            "loss=0.04963762313127518 batch_id=429:  92%|█████████▏| 430/469 [00:19<00:01, 22.56it/s]\u001b[A\n",
            "loss=0.05694776028394699 batch_id=430:  92%|█████████▏| 430/469 [00:19<00:01, 22.56it/s]\u001b[A\n",
            "loss=0.044000040739774704 batch_id=431:  92%|█████████▏| 430/469 [00:19<00:01, 22.56it/s]\u001b[A\n",
            "loss=0.12396694719791412 batch_id=432:  92%|█████████▏| 430/469 [00:19<00:01, 22.56it/s] \u001b[A\n",
            "loss=0.12396694719791412 batch_id=432:  92%|█████████▏| 433/469 [00:19<00:01, 22.82it/s]\u001b[A\n",
            "loss=0.0813257172703743 batch_id=433:  92%|█████████▏| 433/469 [00:19<00:01, 22.82it/s] \u001b[A\n",
            "loss=0.07371687144041061 batch_id=434:  92%|█████████▏| 433/469 [00:19<00:01, 22.82it/s]\u001b[A\n",
            "loss=0.08961109071969986 batch_id=435:  92%|█████████▏| 433/469 [00:19<00:01, 22.82it/s]\u001b[A\n",
            "loss=0.08961109071969986 batch_id=435:  93%|█████████▎| 436/469 [00:19<00:01, 22.82it/s]\u001b[A\n",
            "loss=0.17985489964485168 batch_id=436:  93%|█████████▎| 436/469 [00:20<00:01, 22.82it/s]\u001b[A\n",
            "loss=0.08107659220695496 batch_id=437:  93%|█████████▎| 436/469 [00:20<00:01, 22.82it/s]\u001b[A\n",
            "loss=0.06993359327316284 batch_id=438:  93%|█████████▎| 436/469 [00:20<00:01, 22.82it/s]\u001b[A\n",
            "loss=0.06993359327316284 batch_id=438:  94%|█████████▎| 439/469 [00:20<00:01, 22.53it/s]\u001b[A\n",
            "loss=0.030105896294116974 batch_id=439:  94%|█████████▎| 439/469 [00:20<00:01, 22.53it/s]\u001b[A\n",
            "loss=0.0718693882226944 batch_id=440:  94%|█████████▎| 439/469 [00:20<00:01, 22.53it/s]  \u001b[A\n",
            "loss=0.06146145239472389 batch_id=441:  94%|█████████▎| 439/469 [00:20<00:01, 22.53it/s]\u001b[A\n",
            "loss=0.06146145239472389 batch_id=441:  94%|█████████▍| 442/469 [00:20<00:01, 21.73it/s]\u001b[A\n",
            "loss=0.06794839352369308 batch_id=442:  94%|█████████▍| 442/469 [00:20<00:01, 21.73it/s]\u001b[A\n",
            "loss=0.07868769764900208 batch_id=443:  94%|█████████▍| 442/469 [00:20<00:01, 21.73it/s]\u001b[A\n",
            "loss=0.06343189626932144 batch_id=444:  94%|█████████▍| 442/469 [00:20<00:01, 21.73it/s]\u001b[A\n",
            "loss=0.06343189626932144 batch_id=444:  95%|█████████▍| 445/469 [00:20<00:01, 21.74it/s]\u001b[A\n",
            "loss=0.016239915043115616 batch_id=445:  95%|█████████▍| 445/469 [00:20<00:01, 21.74it/s]\u001b[A\n",
            "loss=0.017937764525413513 batch_id=446:  95%|█████████▍| 445/469 [00:20<00:01, 21.74it/s]\u001b[A\n",
            "loss=0.09357640892267227 batch_id=447:  95%|█████████▍| 445/469 [00:20<00:01, 21.74it/s] \u001b[A\n",
            "loss=0.09357640892267227 batch_id=447:  96%|█████████▌| 448/469 [00:20<00:00, 21.51it/s]\u001b[A\n",
            "loss=0.042040202766656876 batch_id=448:  96%|█████████▌| 448/469 [00:20<00:00, 21.51it/s]\u001b[A\n",
            "loss=0.061211612075567245 batch_id=449:  96%|█████████▌| 448/469 [00:20<00:00, 21.51it/s]\u001b[A\n",
            "loss=0.10165143013000488 batch_id=450:  96%|█████████▌| 448/469 [00:20<00:00, 21.51it/s] \u001b[A\n",
            "loss=0.10165143013000488 batch_id=450:  96%|█████████▌| 451/469 [00:20<00:00, 22.04it/s]\u001b[A\n",
            "loss=0.11703316122293472 batch_id=451:  96%|█████████▌| 451/469 [00:20<00:00, 22.04it/s]\u001b[A\n",
            "loss=0.15239864587783813 batch_id=452:  96%|█████████▌| 451/469 [00:20<00:00, 22.04it/s]\u001b[A\n",
            "loss=0.10713393986225128 batch_id=453:  96%|█████████▌| 451/469 [00:20<00:00, 22.04it/s]\u001b[A\n",
            "loss=0.10713393986225128 batch_id=453:  97%|█████████▋| 454/469 [00:20<00:00, 22.15it/s]\u001b[A\n",
            "loss=0.029629401862621307 batch_id=454:  97%|█████████▋| 454/469 [00:20<00:00, 22.15it/s]\u001b[A\n",
            "loss=0.11197330802679062 batch_id=455:  97%|█████████▋| 454/469 [00:20<00:00, 22.15it/s] \u001b[A\n",
            "loss=0.03800459951162338 batch_id=456:  97%|█████████▋| 454/469 [00:20<00:00, 22.15it/s]\u001b[A\n",
            "loss=0.03800459951162338 batch_id=456:  97%|█████████▋| 457/469 [00:20<00:00, 22.29it/s]\u001b[A\n",
            "loss=0.05263705551624298 batch_id=457:  97%|█████████▋| 457/469 [00:20<00:00, 22.29it/s]\u001b[A\n",
            "loss=0.020625758916139603 batch_id=458:  97%|█████████▋| 457/469 [00:20<00:00, 22.29it/s]\u001b[A\n",
            "loss=0.04956841841340065 batch_id=459:  97%|█████████▋| 457/469 [00:21<00:00, 22.29it/s] \u001b[A\n",
            "loss=0.04956841841340065 batch_id=459:  98%|█████████▊| 460/469 [00:21<00:00, 22.43it/s]\u001b[A\n",
            "loss=0.0946035087108612 batch_id=460:  98%|█████████▊| 460/469 [00:21<00:00, 22.43it/s] \u001b[A\n",
            "loss=0.13924546539783478 batch_id=461:  98%|█████████▊| 460/469 [00:21<00:00, 22.43it/s]\u001b[A\n",
            "loss=0.05507134646177292 batch_id=462:  98%|█████████▊| 460/469 [00:21<00:00, 22.43it/s]\u001b[A\n",
            "loss=0.05507134646177292 batch_id=462:  99%|█████████▊| 463/469 [00:21<00:00, 21.82it/s]\u001b[A\n",
            "loss=0.08993887901306152 batch_id=463:  99%|█████████▊| 463/469 [00:21<00:00, 21.82it/s]\u001b[A\n",
            "loss=0.057959746569395065 batch_id=464:  99%|█████████▊| 463/469 [00:21<00:00, 21.82it/s]\u001b[A\n",
            "loss=0.03854840621352196 batch_id=465:  99%|█████████▊| 463/469 [00:21<00:00, 21.82it/s] \u001b[A\n",
            "loss=0.03854840621352196 batch_id=465:  99%|█████████▉| 466/469 [00:21<00:00, 22.04it/s]\u001b[A\n",
            "loss=0.07200517505407333 batch_id=466:  99%|█████████▉| 466/469 [00:21<00:00, 22.04it/s]\u001b[A\n",
            "loss=0.052864644676446915 batch_id=467:  99%|█████████▉| 466/469 [00:21<00:00, 22.04it/s]\u001b[A\n",
            "loss=0.02085883915424347 batch_id=468:  99%|█████████▉| 466/469 [00:21<00:00, 22.04it/s] \u001b[A\n",
            "loss=0.02085883915424347 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 23.39it/s]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0661, Accuracy: 9780/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}